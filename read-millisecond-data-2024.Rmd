---
title: "Read data from Viikki field weather station"
subtitle: "50-ms-interval logging with Campbell Scientific CR6"
author: "Pedro J. Aphalo"
date: "`r Sys.Date()`"
output: html_document
---

Note: this data set is large. Processing 1 month worth of data may require more
than 8GB RAM, and running taking several tens of minutes to one hour on a fast
workstation.

Clean up workspace!

```{r}
rm(list = ls(pattern = "*"))
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, echo = TRUE)
make.plots <- FALSE # make plots or skip them
```

# News

[2024-04-15] 

Logging at 50 ms started with only the CS/Apogee PAR sensor. Logging active
from minute 5 to minute 10 in each hour, as long as PAR > 1 umol m-2 s-1.

For 50 ms to work the notch frequency had to be increased to 500 Hz.
This decreased the noise-reducing averaging per individual data 
acquisition+conversion from approximately 200 ms to 20 ms.

[2024-08-09] 

Connected 5 sglux fast (< 5 ms time constant) sensors in place of Campbell 107 
thermistors. Added sampling if these sensors to the Millisecond table.

Logging at 50 ms continue with the CS/Apogee PAR sensor and the 5 sglex sensors. 
Logging active from minute 5 to minute 10 in each 1/4 hour, as long as PAR > 1 
umol m-2 s-1.

Logging started with the 5 sglux sensors not powered, which gave a slightly 
negative reading. Some time later I put black caps to measure the dark signal
getting a very small positive reading,

Later I moved the 5 sensors into a rather open faba bean canopy in the inter 
row.

# Import and preprocessing of the data logged at 1 s intervals

```{r}
library(readr)
library(lubridate)
library(photobiology)
library(photobiologyInOut)
library(ggspectra)
library(ggdensity)
library(lubridate)
library(dplyr)
library(ggpp)
```

We store the coordinates of the station to within < 5 m, plus any possible error in Google Maps image layer coordinates.

```{r}
viikki_bio3.geo <- data.frame(lon = 25.019212, lat = 60.226805, address = "BIO3, Viikki")
locale_UTC <- locale(tz = "UTC")
```

We read the 50-ms-interval data for whole period. These data are only for and for 5 min for each hour, from 5 to 20 min past the hour. 

When downloading data from the logger we append it to the existing file. A new file is started when the data tables change, i.e., when the edits to the logger program modifies the structure of a data table. This resetting is done per table. From time to time, such as once per year, I force a flushing of old data, even when data tables have not changed. 

```{r}
# read previous
# millisecond_raw.tb <- read_csi_dat(file = "data-latest/Viikki Tower_TableMilliSecond.dat.backup", locale = locale_UTC)

# read current
millisecond_raw.tb <- read_csi_dat(file = "data-latest/Viikki Tower_TableMilliSecond.dat", locale = locale_UTC)

millisecond_raw.tb
nrow(millisecond_raw.tb)
ncol(millisecond_raw.tb)

range(millisecond_raw.tb$TIMESTAMP)
```

```{r}
colnames(millisecond_raw.tb)
```

```{r}
cat(comment(millisecond_raw.tb))
```

Sometimes after changes in the logger program data already downloaded has been
appended to files, leading to duplicate rows.

```{r}
rle.check <- rle(sort(as.numeric(millisecond_raw.tb$TIMESTAMP)))
duplicates <- sum(rle.check$lengths > 1)
if (duplicates > 0L) {
  message("Found ", duplicates, " duplicated rows in data.")
  millisecond_raw.tb <- distinct(millisecond_raw.tb, TIMESTAMP, .keep_all = TRUE)
}
```


```{r}
# Handle also old "slow" logger program
if (! all(c("UVBf_Den", "UVAf_Den", "UVA1f_Den", "Bluef_Den", "Greenf_Den") %in%
          colnames(millisecond_raw.tb))) {
  millisecond_raw.tb |>
    mutate(UVBf_Den = NA_real_,
           UVAf_Den = NA_real_,
           UVA1f_Den = NA_real_,
           Bluef_Den = NA_real_,
           Greenf_Den = NA_real_) -> millisecond_raw.tb
}
```

To force TZ with 'lubridate' we would need to be sure that daylight saving times have been automatically set. This is not the case so we need first to subtract 2 h from UTC + 2h time, to convert to true UTC and then do calculations based on this. We re-express all times in UTC year round.

```{r}
millisecond_raw.tb[["TIMESTAMP"]][1]
millisecond_raw.tb[["TIMESTAMP"]][nrow(millisecond_raw.tb)]
tz(millisecond_raw.tb[["TIMESTAMP"]][1])
millisecond_raw.tb[["TIMESTAMP_UTC"]] <-  millisecond_raw.tb[["TIMESTAMP"]] - hours(2) # UTC + 2h -> UTC
millisecond_raw.tb[["TIMESTAMP_UTC"]][1]
millisecond_raw.tb[["TIMESTAMP_UTC"]][nrow(millisecond_raw.tb)]
tz(millisecond_raw.tb[["TIMESTAMP_UTC"]][1])
```

In the next code chunk we do the following:

a. Extract components of date time.
b. Compute solar time and sun elevation and azimuth.
FALSE) or not (sunny = TRUE).

```{r}
# NEW CR6 program with fast sglux sensors
millisecond_raw.tb |>
  select(-RECORD) |>
  filter(!is.na(TIMESTAMP)) |>
  rename(time = TIMESTAMP_UTC,
         time_utcp2h = TIMESTAMP
         ) |>
  mutate(year = year(time),
         month_of_year = month(time),
         month_name = factor(month_of_year, levels = 12:1, labels = rev(month.name)),
         month_of_year_utcp2h = month(time_utcp2h),
         month_name_utcp2h = factor(month_of_year_utcp2h, levels = 12:1, labels = rev(month.name)),
         week_of_year = week(time),
         week_of_year_utcp2h = week(time_utcp2h),
         day_of_year = yday(time),
         day_of_year_utcp2h = yday(time_utcp2h),
         time_of_day_utc = as_tod(time),
         time_of_day_utcp2h = as_tod(time_utcp2h),
         solar_time_h = solar_time(time, 
                                   geocode = viikki_bio3.geo),
         solar_time_s = solar_time(time, 
                                   geocode = viikki_bio3.geo, 
                                   unit.out = "seconds"),
         sun_elevation = sun_elevation(time, 
                                       geocode = viikki_bio3.geo, 
                                       use.refraction = TRUE,
                                       tz = "UTC"),
         sun_azimuth = sun_azimuth(time, 
                                   geocode = viikki_bio3.geo, 
                                   use.refraction = TRUE,
                                   tz = "UTC")) -> millisecond.tb
```

```{r}
# first look

millisecond.tb |>
  head()
millisecond.tb |>
  tail()
```

```{r}
# summaries for manual checks
colnames(millisecond.tb)
millisecond.tb |>
  summarise(across(PAR_Den_CS:sun_azimuth, function(x) {class(x)[[1]]},  .names = "{.col}_cls"))

millisecond.tb |>
  summarise(across(PAR_Den_CS:sun_azimuth, function(x) {sum(is.na(x))},  .names = "{.col}_cls")) |>
  t() |> max()

sum(is.na(millisecond.tb$PAR_Den_CS))

millisecond.tb |>
  filter(!is.na(PAR_Den_CS)) -> millisecond.tb

millisecond.tb |>
  reframe(across(PAR_Den_CS:month_of_year, range, .names = "{.col}"),
          across(week_of_year_utcp2h, range, .names = "{.col}"),
          across(week_of_year:sun_azimuth, range, .names = "{.col}"))

millisecond.tb |>
  reframe(across(PAR_Den_CS:month_of_year, range, .names = "{.col}"),
          across(week_of_year_utcp2h, range, .names = "{.col}"),
          across(week_of_year:sun_azimuth, range, .names = "{.col}"),
          n = n(),
          .by = month_name)

```

 In 2024, the fast sglux sensors were at times inside the faba beans canopy and part of the time in the open as reference. We add this information to the data frame.
 
```{r}
millisecond.tb |>
    mutate(f_in_the_open = (time > ymd_hm("2024-08-20 13:32") & time < ymd_hm("2024-08-20 14:15")) |
           (time > ymd_hm("2024-08-25 11:10") & time < ymd_hm("2024-08-20 11:24")) |
           (time > ymd_hm("2024-09-02 12:05") & time < ymd_hm("2024-09-02 12:22")) |
           (time > ymd_hm("2024-10-01 0:0")),
           f_in_canopy = !f_in_the_open & 
             (time > ymd_hm("2024-08-10 0:0") & time < ymd_hm("2024-09-30 0:0")),
           f_unknown = !xor(f_in_the_open, f_in_canopy) # after end of experiment possibly dirty or under snow.
    ) -> millisecond.tb

# we use rle for performance
find_time_slices <- function(x) {
  x <- as.numeric(x) # convert time to running seconds
  boundary <- c(TRUE, diff(x) > 1) # time gaps > 1 second
  # length of gaps should be always == 1 with TRUE indicating first row of each time slice, but we do not assume this
  # length of slices could vary because of failures to measure, although unlikely
  boundary_rle <- rle(boundary)
  cat("length in: ",  length(x), "\n",
      "RLE sum lengths: ", sum(boundary_rle$lengths), "\n")
  # RLE computes these lengths, remove the gaps and add length of gap to the length of each slice
  boundary_rle$lengths <- boundary_rle$lengths[!boundary_rle$value] + 
    boundary_rle$lengths[boundary_rle$value]
  # we replace the logical values with running numbers
  boundary_rle$values <- 1:length(boundary_rle$lengths)
  cat("RLE edited sum lengths: ", sum(boundary_rle$lengths), "\n")
  cat("Number of slices:", length(boundary_rle$lengths), "\n")
  cat("Values per slice in range:", range(boundary_rle$lengths), "\n")
  cat("Number of short slices:", sum(boundary_rle$lengths < 0.9 * max(boundary_rle$lengths)), "\n")
  # expand the RLE obtaining one distinct index for each slice
  inverse.rle(boundary_rle)
}

millisecond.tb |>
    mutate(time_slice = find_time_slices(time) # time gaps > 1 second start new slice
           ) -> millisecond.tb

# irradiance values acquired per sensor in millions
format(nrow(millisecond.tb), big.mark = ",")
format(sum(millisecond.tb$time_slice & millisecond.tb$f_in_the_open), big.mark = ",")
format(sum(millisecond.tb$time_slice & millisecond.tb$f_in_canopy), big.mark = ",")
format(sum(millisecond.tb$time_slice & millisecond.tb$f_unknown), big.mark = ",")

# number of time slices with measurements
length(unique(millisecond.tb[millisecond.tb$f_in_the_open, ]$time_slice))
length(unique(millisecond.tb[millisecond.tb$f_in_canopy, ]$time_slice))
length(unique(millisecond.tb[millisecond.tb$f_unknown, ]$time_slice))

range(rle(millisecond.tb[millisecond.tb$f_in_canopy , ]$time_slice)$lengths)
range(rle(millisecond.tb[millisecond.tb$f_in_the_open , ]$time_slice)$lengths)

length(unique(millisecond.tb$time_slice)) == max(millisecond.tb$time_slice)
```


```{r}
colnames(millisecond.tb)

series.name <- paste(year(millisecond_raw.tb$TIMESTAMP[1]),
                  "_", month(millisecond_raw.tb$TIMESTAMP[1]), 
                  "_", day(millisecond_raw.tb$TIMESTAMP[1]), ".tb", sep = "")
obj.name <- paste("millisecond_", series.name, sep = "")

assign(obj.name, millisecond.tb)

save(list = obj.name, file = paste("data-rda-partial/", obj.name, ".rda", sep = ""))
```

## Compute means by slice

```{r, eval=TRUE}
millisecond.tb |>
  group_by(time_slice) |>
  summarize(across(f_in_the_open:f_unknown, first, .names = "{.col}"),
            across(time, first, .names = "{.col}_start_utc"),
            across(time, last, .names = "{.col}_end_utc"),
            time_step = max(median(as.numeric(diff(time))) * 1e3), # in ms
            across(year:day_of_year_utcp2h, first, .names = "{.col}"),
            across(time_of_day_utc:sun_azimuth, median, .names = "{.col}"),
            across(PAR_Den_CS:Greenf_Den, mean, .names = "{.col}"),
            n = n()) |>
  ungroup() |>
# logging depends on two conditions: time and PAR > 1 umol
# late and early in the day PAR varies around 1 umol and part of the slice is skipped
  mutate(slice_duration = time_end_utc - time_start_utc,
         good_slice = slice_duration > 290 & # seconds
           slice_duration < 310 & # seconds
           round(time_step) <= 51 & # milliseconds, no skips
           n > 5000) -> slice_avg.tb

nrow(slice_avg.tb)
colnames(slice_avg.tb)

sum(!slice_avg.tb$good_slice)
sum(slice_avg.tb$good_slice)
sum(slice_avg.tb$good_slice & slice_avg.tb$f_in_canopy)
sum(slice_avg.tb$good_slice & slice_avg.tb$f_in_the_open)
sum(slice_avg.tb$good_slice & slice_avg.tb$f_unknown)

range(slice_avg.tb$time_step)
range(slice_avg.tb$n)
range(slice_avg.tb$slice_duration)
sum(slice_avg.tb$n > 5000) # expected n = 6000 during 5 min at 50 ms
sum(slice_avg.tb$n <= 5000)
sum(slice_avg.tb$slice_duration < seconds(240)) # slices < 4 min
sum(slice_avg.tb$slice_duration < seconds(299)) # slices < 4 min 59 s
sum(slice_avg.tb$slice_duration > seconds(301)) # slices > 5 min 1 s
```
```{r, eval=TRUE}
obj.name <- paste("slice_calc_", series.name, sep = "")
assign(obj.name, slice_avg.tb)

save(list = obj.name, file = paste("data-rda-partial/", obj.name, ".rda", sep = ""))
```

## Compute 1 s mean for cross-calibration

```{r, eval=TRUE}
time_step <- mean(as.numeric(diff(millisecond.tb$time[1:10]))) # in seconds
values_second <- round(1/time_step, 0)
values_second_threshold <- values_second * 3 / 4
  
millisecond.tb |>
  mutate(time.second = trunc(time, units = "secs") + seconds(1)) |> # match FMI timing
  group_by(time.second) |>
  summarize(across(time, first, .names = "{.col}_start_utc"),
            # across(time, last, .names = "{.col}_end_utc"),
            across(year:day_of_year_utcp2h, first, .names = "{.col}"),
            across(time_of_day_utc:sun_azimuth, median, .names = "{.col}"),
            across(PAR_Den_CS:Greenf_Den, mean, .names = "{.col}"),
            n = n(),
            incomplete.data = n < values_second,
            bad.data = n < values_second_threshold) |>
  ungroup() |>
  mutate(time = as.POSIXct(time.second)) |>
  select(-time.second) -> second_avg.tb

print(second_avg.tb)
colnames(second_avg.tb)
```



```{r, eval=TRUE}
second_avg.tb |>
  head()
second_avg.tb |>
  tail()
```

```{r, eval=TRUE}
obj.name <- paste("second_calc_", series.name, sep = "")
assign(obj.name, second_avg.tb)

save(list = obj.name, file = paste("data-rda-partial/", obj.name, ".rda", sep = ""))
```

## Compute 1 min mean for cross-calibration

```{r, eval=TRUE}
values_minute <- values_second * 60
values_minute_threshold <- values_minute * 3 / 4

millisecond.tb |>
  mutate(time.minute = trunc(time, units = "mins") + minutes(1)) |> # match FMI timing
  group_by(time.minute) |>
  summarize(across(time, first, .names = "{.col}_start_utc"),
            # across(time, last, .names = "{.col}_end_utc"),
            across(year:day_of_year, first, .names = "{.col}"),
            across(time_of_day_utc:sun_azimuth, median, .names = "{.col}"),
            across(PAR_Den_CS:Greenf_Den, mean, .names = "{.col}"),
            n = n(),
            incomplete.data = n < values_minute,
            bad.data = n < values_minute_threshold) |>
  ungroup() |>
  mutate(time = as.POSIXct(time.minute)) |>
  select(-time.minute) -> minute_avg.tb

print(minute_avg.tb)
colnames(minute_avg.tb)
```


```{r, eval=TRUE}
nrow(minute_avg.tb)

sum(minute_avg.tb$bad.data)
sum(minute_avg.tb$incomplete.data)

minute_avg.tb |>
  head()
minute_avg.tb |>
  tail()
```

```{r, eval=TRUE}
obj.name <- paste("minute_fast_calc_", series.name, sep = "")
assign(obj.name, minute_avg.tb)

save(list = obj.name, file = paste("data-rda-partial/", obj.name, ".rda", sep = ""))
```

## Plots

Density distribution plot. During daytime irradiance varies. The millisecond data are acquired only when PAR_Den_CS > 1 umol m-2 s-1 and only for a fraction of each hour so as to keep the data set size manageable. As irradiance around night time stays close to zero for several hours at high latitude these values are the most frequent ones. In 2024, the fast sglux sensors were at times inside the faba beans canopy and part of the time in the open as reference.

```{r, eval=make.plots}
millisecond.tb[-1, ] |>
  select(PAR_Den_CS) |> # only variable to be used, to make "gg" object smaller
ggplot(aes(PAR_Den_CS)) +
  stat_density(fill = "grey50") +
  stat_panel_counts(label.x = "center") +
  geom_vline(xintercept = range(millisecond.tb$PAR_Den_CS), colour = "red") +
  scale_x_continuous(limits = function(x) {c(x[1] - 100, x[2] + 20)}) +
  theme_bw()
```

```{r, eval=make.plots}
millisecond.tb[-1, ] |>
  select(PAR_Den_CS) |> # only variable to be used, to make "gg" object smaller
ggplot(aes(PAR_Den_CS + 0.01)) +
  stat_density(fill = "grey50") +
  stat_panel_counts(label.x = "center") +
  geom_vline(xintercept = range(millisecond.tb$PAR_Den_CS), colour = "red") +
  scale_x_log10(limits = function(x) {c(x[1], x[2] + 20)}) +
  theme_bw()
```

```{r, eval=make.plots}
millisecond.tb[-1, ] |>
  select(solar_time_h, month_of_year_utcp2h, PAR_Den_CS) |> # only variables to be used, to make "gg" object smaller
  filter(solar_time_h >= 3 & solar_time_h <= 21 ) |>
ggplot(aes(solar_time_h, PAR_Den_CS)) +
#  geom_line() +
  stat_smooth() +
  scale_x_continuous(breaks = c(0, 6, 12, 18)) +
  expand_limits(y = 0) +
  facet_wrap(facets = vars(month_of_year_utcp2h))
```

```{r, eval=make.plots}
millisecond.tb[-1, ] |>
  select(solar_time_h, month_of_year_utcp2h, PAR_Den_CS) |> # only variable to be used, to make "gg" object smaller
  filter(solar_time_h >= 3 & solar_time_h <= 21 ) |>
ggplot(aes(solar_time_h, PAR_Den_CS)) +
  geom_hdr() +
  stat_smooth() +
  scale_y_log10() +
  expand_limits(y = 0) +
  facet_wrap(facets = vars(month_of_year_utcp2h))
```


```{r, eval=make.plots}
ggplot(millisecond.tb[-1, ], aes(factor(round(solar_time_h, 0)), PAR_Den_CS)) +
#  geom_line() +
  geom_violin() +
  expand_limits(y = 0) +
  facet_wrap(facets = vars(month_name_utcp2h))
```

```{r, eval=make.plots}
subset(millisecond.tb, time > (max(time) - days(1))) |>
ggplot(aes(time, PAR_Den_CS)) +
  geom_point(size = 0.2) +
  expand_limits(y = 0)
```
