---
title: "Read data from Viikki field weather station"
subtitle: "50-ms-interval logging with Campbell Scientific CR6"
author: "Pedro J. Aphalo"
date: "`r Sys.Date()`"
output: html_document
---

Note: this data set is large. Processing 1 month worth of data may require more
than 8GB RAM, and running taking several tens of minutes to one hour on a fast
workstation.

Clean up workspace!

```{r}
rm(list = ls(pattern = "*"))
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, echo = TRUE)
make.plots <- TRUE # make plots or skip them
```

# News

[2024-04-15] 

Logging at 50 ms started with only the CS/Apogee PAR sensor. Logging active
from minute 5 to minute 10 in each hour, as long as PAR > 1 umol m-2 s-1.

For 50 ms to work the notch frequency had to be increased to 500 Hz.
This decreased the noise-reducing averaging per individual data 
acquisition+conversion from approximately 200 ms to 20 ms.

[2024-08-09] 

Connected 5 sglux fast (< 5 ms time constant) sensors in place of Campbell 107 
thermistors. Added sampling if these sensors to the Millisecond table.

Logging at 50 ms continue with the CS/Apogee PAR sensor and the 5 sglex sensors. 
Logging active from minute 5 to minute 10 in each 1/4 hour, as long as PAR > 1 
umol m-2 s-1.

Logging started with the 5 sglux sensors not powered, which gave a slightly 
negative reading. Some time later I put black caps to measure the dark signal
getting a very small positive reading,

Later I moved the 5 sensors into a rather open faba bean canopy in the inter 
row.

# Import and preprocessing of the data logged at 1 s intervals

```{r}
library(readr)
library(lubridate)
library(photobiology)
library(photobiologyInOut)
library(ggspectra)
library(lubridate)
library(dplyr)
library(ggpp)
```

We store the coordinates of the station to within < 5 m, plus any possible error in Google Maps image layer coordinates.

```{r}
viikki_bio3.geo <- data.frame(lon = 25.019212, lat = 60.226805, address = "BIO3, Viikki")
locale_UTC <- locale(tz = "UTC")
```

We read the 50-ms-interval data for whole period. These data are only for and for 5 min for each hour, from 5 to 20 min past the hour. 

When downloading data from the logger we append it to the existing file. A new file is started when the data tables change, i.e., when the edits to the logger program modifies the structure of a data table. This resetting is done per table. From time to time, such as once per year, I force a flushing of old data, even when data tables have not changed. 

```{r}
# read previous
# millisecond_raw.tb <- read_csi_dat(file = "data-latest/Viikki Tower_TableMilliSecond.dat.backup", locale = locale_UTC)

# read current
millisecond_raw.tb <- read_csi_dat(file = "data-latest/Viikki Tower_TableMilliSecond.dat", locale = locale_UTC)

millisecond_raw.tb
nrow(millisecond_raw.tb)
ncol(millisecond_raw.tb)

range(millisecond_raw.tb$TIMESTAMP)
```

```{r}
colnames(millisecond_raw.tb)
```

```{r}
cat(comment(millisecond_raw.tb))
```

Sometimes after changes in the logger program data already downloaded has been
appended to files, leading to duplicate rows.

```{r}
rle.check <- rle(sort(as.numeric(millisecond_raw.tb$TIMESTAMP)))
duplicates <- sum(rle.check$lengths > 1)
if (duplicates > 0L) {
  message("Found ", duplicates, " duplicated rows in data.")
  millisecond_raw.tb <- distinct(millisecond_raw.tb, TIMESTAMP, .keep_all = TRUE)
}
```

To force TZ with 'lubridate' we would need to be sure that daylight saving times have been automatically set. This is not the case so we need first to subtract 2 h from UTC + 2h time, to convert to true UTC and then do calculations based on this. We re-express all times in UTC year round.

```{r}
millisecond_raw.tb[["TIMESTAMP"]][1]
millisecond_raw.tb[["TIMESTAMP"]][nrow(millisecond_raw.tb)]
tz(millisecond_raw.tb[["TIMESTAMP"]][1])
millisecond_raw.tb[["TIMESTAMP"]] <-  millisecond_raw.tb[["TIMESTAMP"]] - hours(2) # UTC + 2h -> UTC
millisecond_raw.tb[["TIMESTAMP"]][1]
millisecond_raw.tb[["TIMESTAMP"]][nrow(millisecond_raw.tb)]
tz(millisecond_raw.tb[["TIMESTAMP"]][1])
```

In the next code chunk we do the following:

a. Extract components of date time.
b. Compute solar time and sun elevation and azimuth.
FALSE) or not (sunny = TRUE).

```{r}
  # NEW CR6 program
millisecond_raw.tb %>%
  filter(!is.na(TIMESTAMP)) %>%
  rename(time = TIMESTAMP) |>
  mutate(year = year(time),
         month_of_year = month(time),
         month_name = factor(month_of_year, levels = 12:1, labels = rev(month.name)),
         week_of_year = week(time),
         day_of_year = yday(time),
         time_of_day_utc = as_tod(time),
         solar_time_h = solar_time(time, 
                                   geocode = viikki_bio3.geo),
         solar_time_s = solar_time(time, 
                                   geocode = viikki_bio3.geo, 
                                   unit.out = "seconds"),
         sun_elevation = sun_elevation(time, 
                                       geocode = viikki_bio3.geo, 
                                       use.refraction = TRUE,
                                       tz = "UTC"),
         sun_azimuth = sun_azimuth(time, 
                                   geocode = viikki_bio3.geo, 
                                   use.refraction = TRUE,
                                   tz = "UTC")) -> millisecond.tb
```

```{r}
colnames(millisecond.tb)
```

```{r}

series.name <- paste(year(millisecond_raw.tb$TIMESTAMP[1]),
                  "_", month(millisecond_raw.tb$TIMESTAMP[1]), 
                  "_", day(millisecond_raw.tb$TIMESTAMP[1]), ".tb", sep = "")
obj.name <- paste("millisecond_", series.name, sep = "")

assign(obj.name, millisecond.tb)

save(list = obj.name, file = paste("data-rda-partial/", obj.name, ".rda", sep = ""))
```


```{r}
millisecond.tb %>%
  head()
millisecond.tb %>%
  tail()
```

## Compute 1 s mean for cross-calibration

```{r, eval=TRUE}
# second.tb$time.minute <- floor_date(second.tb$time, unit = "minute")

millisecond.tb |>
  mutate(time.second = trunc(time, units = "secs") + seconds(1)) |> # match FMI timing
  group_by(time.second) |>
  summarize(across(time, first, .names = "{.col}_start"),
            across(time, last, .names = "{.col}_end"),
            across(year:day_of_year, first, .names = "{.col}"),
            across(time_of_day_utc:sun_azimuth, median, .names = "{.col}"),
            across(PAR_Den_CS:Greenf_Den, mean, .names = "{.col}"),
            n = n(),
            incomplete.data = n < 20,
            bad.data = n < 15) |>
  ungroup() |>
  mutate(time.second = as.POSIXct(time.second)) |>
  rename(time = time.second) -> second_avg.tb

print(second_avg.tb)
colnames(second_avg.tb)
```



```{r, eval=TRUE}
second_avg.tb %>%
  head()
second_avg.tb %>%
  tail()
```

```{r, eval=TRUE}
obj.name <- paste("second_calc_", series.name, sep = "")
assign(obj.name, second_avg.tb)

save(list = obj.name, file = paste("data-rda-partial/", obj.name, ".rda", sep = ""))
```

## Compute 1 min mean for cross-calibration

```{r, eval=TRUE}
# second.tb$time.minute <- floor_date(second.tb$time, unit = "minute")

millisecond.tb |>
  mutate(time.minute = trunc(time, units = "mins") + minutes(1)) |> # match FMI timing
  group_by(time.minute) |>
  summarize(across(time, first, .names = "{.col}_start"),
            across(time, last, .names = "{.col}_end"),
            across(year:day_of_year, first, .names = "{.col}"),
            across(time_of_day_utc:sun_azimuth, median, .names = "{.col}"),
            across(PAR_Den_CS:Greenf_Den, mean, .names = "{.col}"),
            n = n(),
            incomplete.data = n < 20 * 60,
            bad.data = n < 15 * 60) |>
  ungroup() |>
  mutate(time.minute = as.POSIXct(time.minute)) |>
  rename(time = time.minute) -> minute_avg.tb

print(minute_avg.tb)
colnames(minute_avg.tb)
```



```{r, eval=TRUE}
minute_avg.tb %>%
  head()
minute_avg.tb %>%
  tail()
```

```{r, eval=TRUE}
obj.name <- paste("minute_fast_calc_", series.name, sep = "")
assign(obj.name, minute_avg.tb)

save(list = obj.name, file = paste("data-rda-partial/", obj.name, ".rda", sep = ""))
```

## Plots

```{r, eval=make.plots}
ggplot(millisecond.tb[-1, ], aes(PAR_Den_CS)) +
  stat_density(fill = "grey50") +
  stat_panel_counts(label.x = "center") +
  geom_vline(xintercept = range(millisecond.tb$PAR_Den_CS), colour = "red") +
  scale_x_continuous(limits = function(x) {c(x[1] - 100, x[2] + 20)}) +
  theme_bw()
```

```{r, eval=make.plots}
ggplot(millisecond.tb[-1, ], aes(time, PAR_Den_CS)) +
#  geom_line() +
  geom_point(size = 0.2) +
  expand_limits(y = 0)
```

```{r, eval=make.plots}
subset(millisecond.tb, time > (max(time) - days(1))) |>
ggplot(aes(time, PAR_Den_CS)) +
  geom_point(size = 0.2) +
  expand_limits(y = 0)
```
