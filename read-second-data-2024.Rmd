---
title: "Read data from Viikki field weather station"
subtitle: "One-second-interval logging with Campbell Scientific CR6"
author: "Pedro J. Aphalo"
date: "`r Sys.Date()`"
output: html_document
---

Clean up workspace!

```{r}
rm(list = ls(pattern = "*"))
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, echo = TRUE)
```

# News

Logging started on 2024-04-12 at 5 s, and rate increased to 1 s on 2024-04-15.

For 1 s to work the notch frequency had to be increased from 50 Hz to 500 Hz.
This decreased the noise-reducing averaging per individual data 
acquisition+conversion from approximately 200 ms to 20 ms.


# Import and preprocessing of the data logged at 1 s intervals

```{r}
library(readr)
library(lubridate)
library(photobiology)
library(photobiologyInOut)
library(lubridate)
library(dplyr)
```

We read the precomputed interpolation spline function predicting clear-sky diffuse fraction from sun elevation angle. This function was generated using the TUV model simulated spectral irradiance components integrated over PAR.

```{r}
clear_sky_diff_fr <- readRDS("TUV-diffuse-direct-SZA/spline-fun.RDS")
```

We store the coordinates of the station to within < 1 m, plus any possible error in Google Maps image layer coordinates.

```{r}
viikki_bio3.geo <- data.frame(lon = 25.019212, lat = 60.226805, address = "BIO3, Viikki")
locale_UTC <- locale(tz = "UTC")
```

We read the minute interval data for whole periods. When downloading data from the logger we append it to the file. A new file is started when the data tables change, i.e., when the edits to the logger program modifies the structure of a data table. This resetting is done per table. From time to time, such as once per year, I force a flushing of old data, even when data tables have not changed. 

```{r}
second_raw.tb <- read_csi_dat(file = "data-latest/Viikki Tower_TableSecond.dat", locale = locale_UTC)

head(second_raw.tb, 50)
tail(second_raw.tb, 50)
nrow(second_raw.tb)
ncol(second_raw.tb)

range(second_raw.tb$TIMESTAMP)
```

```{r}
colnames(second_raw.tb) <- gsub("_Avg$", "", colnames(second_raw.tb))
colnames(second_raw.tb)
```

```{r}
comment(second_raw.tb) <- gsub("_Avg", "", comment(second_raw.tb))
cat(comment(second_raw.tb))
```

Sometimes after changes in the logger program data already downloaded has been
appended to files, leading to duplicate rows.

```{r}
rle.check <- rle(sort(as.numeric(second_raw.tb$TIMESTAMP)))
duplicates <- sum(rle.check$lengths > 1)
if (duplicates > 0L) {
  message("Found ", duplicates, " duplicated rows in data.")
  second_raw.tb <- distinct(second_raw.tb, TIMESTAMP, .keep_all = TRUE)
}

head(second_raw.tb, 50)
tail(second_raw.tb, 50)
```

TIMESTAMP values are as logged. Since 2021, i.e., well before the start of this faster measurements has been kept within 1-5 seconds of UTC + 2 h. As a precaution, we check anyway for forward and backward gaps in the time series with durations between 30 and 90 min.

```{r}
which(diff(second_raw.tb$TIMESTAMP) > minutes(30) &
        diff(second_raw.tb$TIMESTAMP) < minutes(90)) -> clock.forward.selector
clock.forward.selector
diff(second_raw.tb$TIMESTAMP)[clock.forward.selector]
second_raw.tb$TIMESTAMP[clock.forward.selector]
```


To force TZ with 'lubridate' we would need to be sure that daylight saving times have been automatically set. This is not the case so we need first to subtract 2 h from UTC + 2h time, to convert to true UTC and then do calculations based on this. We re-express all times in UTC year round.

```{r}
second_raw.tb[["TIMESTAMP"]][1]
second_raw.tb[["TIMESTAMP"]][nrow(second_raw.tb)]
tz(second_raw.tb[["TIMESTAMP"]][1])
second_raw.tb[["TIMESTAMP"]] <-  second_raw.tb[["TIMESTAMP"]] - hours(2) # UTC + 2h -> UTC
second_raw.tb[["TIMESTAMP"]][1]
second_raw.tb[["TIMESTAMP"]][nrow(second_raw.tb)]
tz(second_raw.tb[["TIMESTAMP"]][1])

head(second_raw.tb, 50)
tail(second_raw.tb, 50)

```
Except for the Skye R+FR and sglux sensors calibrations are applied in the logger. For these channels we have mV. The calibrations are adjusted later, in all cases. 

In the next code chunk we do the following:

1. Discard negative R and FR irradiances.
2. Compute R:FR only if both R and FR > 0.05 umol m-2 s-1, otherwise mark as NA.
3. Compute diffuse fraction in PAR, but only if diffuse PAR > 5 umol m-2 s-1, otherwise mark as NA.
4. Extract components of date time.
5. Compute solar time and sun elevation and azimuth.
6. Compute the diffuse fraction scaled to the range between clear sky diffuse fraction and 1. With 1 = fully diffuse and 0 = clear-sky difuse fraction.
7. Compute estimate of whether the solar disk is occluded by clouds (sunny = FALSE) or not (sunny = TRUE).

```{r}
  # NEW CR6 program
second_raw.tb |>
  filter(!is.na(TIMESTAMP)) |>
  rename(time = TIMESTAMP) |>
  mutate(red_umol = Red_Den_cal,
         far_red_umol = Far_red_Den_cal,
         PAR_diff_fr_raw = PAR_BF_diff/PAR_BF_tot,
         # PAR readings from BF5 are noisy and start being unreliable at
         # a relatively high diffuse irradiance of 10 umol m-2 s-1.
         PAR_diff_fr = ifelse(PAR_BF_diff < 10,
                              NA_real_,
                              PAR_diff_fr_raw),
         year = year(time),
         month_of_year = month(time),
         month_name = factor(month_of_year, levels = 12:1, labels = rev(month.name)),
         week_of_year = week(time),
         day_of_year = yday(time),
         time_of_day_utc = as_tod(time),
         solar_time_h = solar_time(time, 
                                   geocode = viikki_bio3.geo),
         sun_elevation = sun_elevation(time, 
                                       geocode = viikki_bio3.geo, 
                                       use.refraction = TRUE,
                                       tz = "UTC"),
         sun_azimuth = sun_azimuth(time, 
                                   geocode = viikki_bio3.geo, 
                                   use.refraction = TRUE,
                                   tz = "UTC"),
         PAR_diff_fr_rel = 1 - ((1 - PAR_diff_fr) / (1 - clear_sky_diff_fr(sun_elevation))),
         sunny = PAR_diff_fr_rel < 0.6,
         solar_disk = factor(sunny,
                             levels = c(FALSE, TRUE),
                             labels = c("occluded", "visible"))) -> second.tb

head(second.tb, 50)
tail(second.tb, 50)

```

We set the sunny flag based on the observed relative PAR fraction compared to that expected under clear sky for the same sun elevation. This is approximate but work fairly well as the distribution is markedly bimodal.

```{r}
colnames(second.tb)
second.tb[1, ]
```


```{r}
series.name <- paste(year(second_raw.tb$TIMESTAMP[1]),
                  "_", month(second_raw.tb$TIMESTAMP[1]), 
                  "_", day(second_raw.tb$TIMESTAMP[1]), ".tb", sep = "")
obj.name <- paste("second_", series.name, sep = "")
assign(obj.name, second.tb)

head(second.tb, 50)
tail(second.tb, 50)

save(list = obj.name, file = paste("data-rda-partial/", obj.name, ".rda", sep = ""))
```

Compute minute averages as they are no longer logged. Makes more sense to
compute them before merging to avoid unnecessary recomputation.

```{r, eval=TRUE}
# second.tb$time.minute <- floor_date(second.tb$time, unit = "minute")

second.tb |>
  mutate(time.minute = trunc(time, units = "mins") + minutes(1)) |> # match FMI timing
  group_by(time.minute) |>
  summarize(across(time, first, .names = "{.col}_start"),
            across(time, last, .names = "{.col}_end"),
            across(year:day_of_year, first, .names = "{.col}"),
            across(time_of_day_utc:sun_azimuth, median, .names = "{.col}"),
            across(PAR_Den:PAR_diff_fr, mean, .names = "{.col}"),
            across(PAR_diff_fr_rel:sunny, mean, .names = "{.col}"),
            sun_visible_fr = sum(solar_disk == "visible") / n(),
            solar_disk = ifelse(sun_visible_fr < 0.5, "occluded", "visible"),
            n = n(),
            incomplete.data = n < 120,
            bad.data = n < 100) |>
  mutate(solar_disk = factor(solar_disk)) |>
  ungroup() |>
  mutate(time.minute = as.POSIXct(time.minute)) |>
  rename(time = time.minute) -> minute.tb

print(minute.tb)
colnames(minute.tb)
```


```{r, eval=TRUE}
minute.tb %>%
  head()
minute.tb %>%
  tail()
```

```{r, eval=TRUE}
obj.name <- paste("minute_calc_", series.name, sep = "")
assign(obj.name, minute.tb)

save(list = obj.name, file = paste("data-rda-partial/", obj.name, ".rda", sep = ""))
```

